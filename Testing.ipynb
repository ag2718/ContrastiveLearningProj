{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "from tree import Tree\n",
    "from data import load_data\n",
    "\n",
    "class_tree = None\n",
    "distance_matrix = None\n",
    "\n",
    "# use 'conda install pytorch torchvision -c pytorch' in python env to import; if using Colab probably use pip \n",
    "\n",
    "def precompute_lca_distances(labels, class_tree):\n",
    "    num_labels = len(labels)\n",
    "    lca_matrix = torch.zeros((num_labels, num_labels), dtype=torch.long)\n",
    "    distance_matrix = torch.zeros((num_labels, num_labels), dtype=torch.float)\n",
    "\n",
    "    # Precompute LCAs and distances\n",
    "    for i in range(num_labels):\n",
    "        for j in range(i + 1, num_labels):\n",
    "            lca = class_tree.find_lca(labels[i].item(), labels[j].item())\n",
    "            distance_i = class_tree.find_distance_to_ancestor(labels[i].item(), lca)\n",
    "            distance_j = class_tree.find_distance_to_ancestor(labels[j].item(), lca)\n",
    "            min_distance = min(distance_i, distance_j)\n",
    "            lca_matrix[i, j] = lca\n",
    "            lca_matrix[j, i] = lca\n",
    "            distance_matrix[i, j] = min_distance\n",
    "            distance_matrix[j, i] = min_distance\n",
    "\n",
    "    return distance_matrix\n",
    "\n",
    "# Basic Contrastive Learning Model\n",
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_dim=128):\n",
    "        super(ContrastiveModel, self).__init__()\n",
    "        # Using ResNet backbone since we are using ImageNet and therefore compatible, feel free to change\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        \n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(num_features, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)  # Extract features\n",
    "        embeddings = self.projection_head(features)  # Project embeddings\n",
    "        logits = self.classifier(embeddings)  # Classification logits\n",
    "        return embeddings, logits\n",
    "\n",
    "\n",
    "# Define the NCE Loss\n",
    "class NCELoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07, dist_func_param=1):\n",
    "        super(NCELoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dist_func_param = dist_func_param\n",
    "\n",
    "    def forward(self, embeddings, labels):\n",
    "        global distance_matrix\n",
    "        # Normalize embeddings to unit vectors\n",
    "        embeddings = nn.functional.normalize(embeddings, dim=1)\n",
    "\n",
    "        similarity_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature\n",
    "\n",
    "        # Mask out self-similarity\n",
    "        mask = torch.eye(similarity_matrix.size(0), device=similarity_matrix.device).bool()\n",
    "        similarity_matrix = similarity_matrix.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        # Create targets: positive samples have the same label\n",
    "        labels = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "        #positives = labels.float()\n",
    "        distances = torch.exp(-self.dist_func_param * distance_matrix[labels][:, labels])\n",
    "\n",
    "        # Compute log-softmax and NCE loss\n",
    "        log_prob = nn.functional.log_softmax(similarity_matrix, dim=1)\n",
    "        #loss = -torch.sum(log_prob * positives) / labels.sum() #loss is only how far apart the positives are\n",
    "        loss = -torch.sum(log_prob * distances) / labels.sum() #loss is only how far apart the positives are\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample dataset preparation, making flexible \n",
    "    # TODO: make as function of level of specificity \n",
    "    class SampleDataset(Dataset):\n",
    "        def __init__(self, size=100, num_classes=10, transform=None):\n",
    "            self.size = size\n",
    "            self.num_classes = num_classes\n",
    "            self.transform = transform\n",
    "        \n",
    "        def __len__(self):\n",
    "            return self.size\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            # Random RGB image and label\n",
    "            image = torch.rand(3, 224, 224)\n",
    "            label = idx % self.num_classes\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "\n",
    "    '''\n",
    "    # DataLoader setup\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    dataset = SampleDataset(size=100, num_classes=10, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    num_classes = 10'''\n",
    "\n",
    "    train_dataloader, val_dataloader, dogs, num_total_classes = load_data()\n",
    "    print (\"Dogs\")\n",
    "    print (dogs)\n",
    "    print (\"Num total classes\")\n",
    "    print (num_total_classes)\n",
    "    class_tree = Tree(dogs)\n",
    "\n",
    "    contrastive_classes_specificity = 2\n",
    "    contrastive_classes = class_tree.nodes_at_depth(contrastive_classes_specificity)\n",
    "    contrastive_class_to_id = {_cls: i for i, _cls in enumerate(contrastive_classes)}\n",
    "\n",
    "    clf_classes_specificity = 1\n",
    "    clf_classes = class_tree.nodes_at_depth(clf_classes_specificity)\n",
    "    clf_class_to_id = {_cls: i for i, _cls in enumerate(clf_classes)}\n",
    "\n",
    "    print (\"Contrastive classes\")\n",
    "    print (contrastive_classes)\n",
    "    print (\"Clf classes\")\n",
    "    print (clf_classes)\n",
    "\n",
    "    num_classes = len(clf_classes)\n",
    "\n",
    "    model = ContrastiveModel(num_classes).cuda()\n",
    "    nce_loss_fn = NCELoss().cuda() # Generally probably add all of this to Colab to use the GPU \n",
    "    classification_loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Precompute distance matrix\n",
    "    distance_matrix = torch.tensor(precompute_lca_distances(np.arange(len(contrastive_classes)), class_tree))\n",
    "\n",
    "    print (\"Distance matrix\")\n",
    "    print (distance_matrix[:10, :10])\n",
    "\n",
    "    # Training loop\n",
    "    # TODO: add in evals or call to eval.py file perhaps? \n",
    "    for epoch in range(2):  # Using two epochs to test but add more epochs\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            images, labels = batch\n",
    "            print (\"Batch\")\n",
    "            print (images.shape, labels.shape)\n",
    "            print (labels)\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            # for each image, find the label at the correct classification level\n",
    "            labels = [class_tree.which_ancestor(label.item(), clf_classes) for label in labels]\n",
    "            # Map labels to their IDs\n",
    "            labels = torch.tensor([clf_class_to_id[label.item()] for label in labels], device=labels.device)\n",
    "            print (\"Mapped labels\")\n",
    "            print (labels)\n",
    "\n",
    "            # Forward pass\n",
    "            embeddings, logits = model(images)\n",
    "            \n",
    "            # Compute losses\n",
    "            nce_loss = nce_loss_fn(embeddings, labels)\n",
    "            classification_loss = classification_loss_fn(logits, labels)\n",
    "            total_loss = nce_loss + classification_loss  # Combined loss\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/2], Loss: {running_loss/len(train_dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/healthy-ml/scratch/abinitha/miniconda3/envs/contrastive/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data/healthy-ml/scratch/abinitha/miniconda3/envs/contrastive/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_2229755/3318987072.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  distance_matrix = torch.tensor(precompute_lca_distances(np.array(contrastive_classes), class_tree), device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1.], device='cuda:0')\n",
      "LABELS: \n",
      "tensor([219, 192, 191, 198, 215, 165, 188, 196, 211, 210, 171, 192, 206, 168,\n",
      "        197, 168, 165, 167, 210, 191, 191, 164, 216, 208, 189, 197, 172, 176,\n",
      "        173, 163])\n",
      "CONTRASTIVE LABELS: \n",
      "tensor([ 2,  7,  7,  7,  2, 12,  7,  7,  2,  2, 12,  7,  2, 12,  7, 12, 12, 12,\n",
      "         2,  7,  7, 12,  2,  2,  7,  7, 12, 12, 12, 12], device='cuda:0')\n",
      "CLASS LABELS: \n",
      "tensor([0, 1, 1, 1, 0, 2, 1, 1, 0, 0, 2, 1, 0, 2, 1, 2, 2, 2, 0, 1, 1, 2, 0, 0,\n",
      "        1, 1, 2, 2, 2, 2], device='cuda:0')\n",
      "LABELS: \n",
      "tensor([209, 191, 208, 212, 189, 188, 178, 162, 178, 173, 185, 169, 178, 191,\n",
      "        213, 200, 177, 162, 219, 212, 162, 195, 167, 205, 194, 167, 187, 197,\n",
      "        201, 199])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLABELS: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m (labels)\n\u001b[0;32m---> 33\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mcuda(), labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# for each image, find the label at the correct classification level\u001b[39;00m\n\u001b[1;32m     36\u001b[0m contr_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([class_tree\u001b[38;5;241m.\u001b[39mwhich_ancestor(label\u001b[38;5;241m.\u001b[39mitem(), clf_classes) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels], device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from tree import Tree\n",
    "from\n",
    "\n",
    "train_dataloader, val_dataloader, dogs, num_total_classes = load_data()\n",
    "class_tree = Tree(dogs)\n",
    "\n",
    "contrastive_classes_specificity = 1\n",
    "contrastive_classes = class_tree.nodes_at_depth(contrastive_classes_specificity)\n",
    "contrastive_class_to_id = {_cls: i for i, _cls in enumerate(contrastive_classes)}\n",
    "\n",
    "clf_classes_specificity = 1\n",
    "clf_classes = class_tree.nodes_at_depth(clf_classes_specificity)\n",
    "clf_class_to_id = {_cls: i for i, _cls in enumerate(clf_classes)}\n",
    "\n",
    "num_classes = len(clf_classes)\n",
    "\n",
    "model = ContrastiveModel(num_classes).to(device=device)\n",
    "nce_loss_fn = NCELoss().cuda() # Generally probably add all of this to Colab to use the GPU \n",
    "classification_loss_fn = nn.CrossEntropyLoss().to(device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Precompute distance matrix\n",
    "distance_matrix = torch.tensor(precompute_lca_distances(np.array(contrastive_classes), class_tree), device=device)\n",
    "print(distance_matrix[2][:2])\n",
    "\n",
    "# Training loop\n",
    "# TODO: add in evals or call to eval.py file perhaps? \n",
    "for epoch in range(2):  # Using two epochs to test but add more epochs\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        images, labels = batch\n",
    "        print(\"LABELS: \")\n",
    "        print (labels)\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        # for each image, find the label at the correct classification level\n",
    "        contr_labels = torch.tensor([class_tree.which_ancestor(label.item(), clf_classes) for label in labels], device=device)\n",
    "        contr_labels_list = [class_tree.which_ancestor(label.item(), clf_classes) for label in labels]\n",
    "        print(\"CONTRASTIVE LABELS: \")\n",
    "        print(contr_labels)\n",
    "\n",
    "        # Map labels to their IDs\n",
    "        class_labels = torch.tensor([clf_class_to_id[contr_label.item()] for contr_label in contr_labels], device=device)\n",
    "        print(\"CLASS LABELS: \")\n",
    "        print(class_labels)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings, logits = model(images)\n",
    "        \n",
    "        # Compute losses\n",
    "        # nce_loss = nce_loss_fn(embeddings, contr_labels)\n",
    "        # nce_loss = nce_loss_fn(embeddings, contr_labels)\n",
    "        classification_loss = classification_loss_fn(logits, labels)\n",
    "\n",
    "        # total_loss = nce_loss + classification_loss  # Combined loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        # total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # running_loss += classification_loss.item()\n",
    "        # running_loss += total_loss.item()\n",
    "\n",
    "    # print(f\"Epoch [{epoch+1}/2], Loss: {running_loss/len(train_dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dogs\n",
      "[1, [2, [3, [156], [215], [216], [219], [217], [218], [220], [221]], [4, [205], [206], [207], [208], [209]], [5, [210], [211]], [6, [212], [213], [214]]], [7, [8, [189], [190]], [9, [179], [180]], [10, [188]], [11, [196], [197], [198]], [191], [193], [181], [182], [192], [194], [195], [184], [183], [185], [186], [199], [200], [201], [202], [203], [187]], [12, [13, [165], [166]], [14, [167], [168]], [15, [171], [172]], [16, [169], [170]], [17, [160], [161], [162], [163], [164], [173], [174], [175], [176], [177], [178], [159]]]]\n",
      "Num total classes\n",
      "4\n",
      "Contrastive classes\n",
      "[3, 4, 5, 6, 8, 9, 10, 11, 191, 193, 181, 182, 192, 194, 195, 184, 183, 185, 186, 199, 200, 201, 202, 203, 187, 13, 14, 15, 16, 17]\n",
      "Clf classes\n",
      "[2, 7, 12]\n",
      "Distance matrix\n",
      "tensor([[0., 1., 1., 1., 2., 2., 2., 2., 2., 2.],\n",
      "        [1., 0., 1., 1., 2., 2., 2., 2., 2., 2.],\n",
      "        [1., 1., 0., 1., 2., 2., 2., 2., 2., 2.],\n",
      "        [1., 1., 1., 0., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 0., 1., 1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 1., 0., 1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 1., 1., 0., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 1., 1., 1., 0., 1., 1.],\n",
      "        [2., 2., 2., 2., 1., 1., 1., 1., 0., 1.],\n",
      "        [2., 2., 2., 2., 1., 1., 1., 1., 1., 0.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2231410/1807714834.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  distance_matrix = torch.tensor(precompute_lca_distances(np.array(contrastive_classes), class_tree))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch\n",
      "torch.Size([30, 3, 224, 224]) torch.Size([30])\n",
      "tensor([219, 192, 191, 198, 215, 165, 188, 196, 211, 210, 171, 192, 206, 168,\n",
      "        197, 168, 165, 167, 210, 191, 191, 164, 216, 208, 189, 197, 172, 176,\n",
      "        173, 163])\n",
      "Mapped labels\n",
      "tensor([0, 1, 1, 1, 0, 2, 1, 1, 0, 0, 2, 1, 0, 2, 1, 2, 2, 2, 0, 1, 1, 2, 0, 0,\n",
      "        1, 1, 2, 2, 2, 2], device='cuda:0')\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m embeddings, logits \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Compute losses\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m nce_loss \u001b[38;5;241m=\u001b[39m nce_loss_fn(embeddings, labels)\n\u001b[1;32m     59\u001b[0m classification_loss \u001b[38;5;241m=\u001b[39m classification_loss_fn(logits, labels)\n\u001b[1;32m     60\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m nce_loss \u001b[38;5;241m+\u001b[39m classification_loss  \u001b[38;5;66;03m# Combined loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/contrastive/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/contrastive/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 83\u001b[0m, in \u001b[0;36mNCELoss.forward\u001b[0;34m(self, embeddings, labels)\u001b[0m\n\u001b[1;32m     81\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#positives = labels.float()\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m distances \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_func_param \u001b[38;5;241m*\u001b[39m distance_matrix[labels][:, labels])\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Compute log-softmax and NCE loss\u001b[39;00m\n\u001b[1;32m     86\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(similarity_matrix, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataloader, val_dataloader, dogs, num_total_classes = load_data()\n",
    "print (\"Dogs\")\n",
    "print (dogs)\n",
    "print (\"Num total classes\")\n",
    "print (num_total_classes)\n",
    "class_tree = Tree(dogs)\n",
    "\n",
    "contrastive_classes_specificity = 2\n",
    "contrastive_classes = class_tree.nodes_at_depth(contrastive_classes_specificity)\n",
    "contrastive_class_to_id = {_cls: i for i, _cls in enumerate(contrastive_classes)}\n",
    "\n",
    "clf_classes_specificity = 1\n",
    "clf_classes = class_tree.nodes_at_depth(clf_classes_specificity)\n",
    "clf_class_to_id = {_cls: i for i, _cls in enumerate(clf_classes)}\n",
    "\n",
    "print (\"Contrastive classes\")\n",
    "print (contrastive_classes)\n",
    "print (\"Clf classes\")\n",
    "print (clf_classes)\n",
    "\n",
    "num_classes = len(clf_classes)\n",
    "\n",
    "model = ContrastiveModel(num_classes).cuda()\n",
    "nce_loss_fn = NCELoss().cuda() # Generally probably add all of this to Colab to use the GPU \n",
    "classification_loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Precompute distance matrix\n",
    "distance_matrix = torch.tensor(precompute_lca_distances(np.array(contrastive_classes), class_tree))\n",
    "\n",
    "print (\"Distance matrix\")\n",
    "print (distance_matrix[:10, :10])\n",
    "\n",
    "# Training loop\n",
    "# TODO: add in evals or call to eval.py file perhaps? \n",
    "for epoch in range(2):  # Using two epochs to test but add more epochs\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        images, labels = batch\n",
    "        print (\"Batch\")\n",
    "        print (images.shape, labels.shape)\n",
    "        print (labels)\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        # for each image, find the label at the correct classification level\n",
    "        labels = [class_tree.which_ancestor(label.item(), clf_classes) for label in labels]\n",
    "        # Map labels to their IDs\n",
    "        labels = torch.tensor([clf_class_to_id[label] for label in labels], device=device)\n",
    "        print (\"Mapped labels\")\n",
    "        print (labels)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings, logits = model(images)\n",
    "        \n",
    "        # Compute losses\n",
    "        nce_loss = nce_loss_fn(embeddings, labels)\n",
    "        classification_loss = classification_loss_fn(logits, labels)\n",
    "        total_loss = nce_loss + classification_loss  # Combined loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/2], Loss: {running_loss/len(train_dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw LABELS:  [219, 192, 191, 198, 215, 165, 188, 196, 211, 210, 171, 192, 206, 168, 197, 168, 165, 167, 210, 191, 191, 164, 216, 208, 189, 197, 172, 176, 173, 163]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'which_ancestor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw LABELS: \u001b[39m\u001b[38;5;124m\"\u001b[39m, labels\u001b[38;5;241m.\u001b[39mtolist())  \u001b[38;5;66;03m# Original labels from the dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mcuda(), labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 8\u001b[0m contr_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([class_tree\u001b[38;5;241m.\u001b[39mwhich_ancestor(label\u001b[38;5;241m.\u001b[39mitem(), clf_classes) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONTRASTIVE LABELS: \u001b[39m\u001b[38;5;124m\"\u001b[39m, contr_labels\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Map to class IDs\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'which_ancestor'"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataloader, val_dataloader, dogs, num_total_classes = load_data()\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    images, labels = batch\n",
    "    print(\"Raw LABELS: \", labels.tolist())  # Original labels from the dataset\n",
    "    images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "    contr_labels = torch.tensor([class_tree.which_ancestor(label.item(), clf_classes) for label in labels], device=device)\n",
    "    print(\"CONTRASTIVE LABELS: \", contr_labels.tolist())\n",
    "\n",
    "    # Map to class IDs\n",
    "    try:\n",
    "        class_labels = torch.tensor([clf_class_to_id[contr_label.item()] for contr_label in contr_labels], device=device)\n",
    "        print(\"CLASS LABELS: \", class_labels.tolist())\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError in class mapping: {e}\")\n",
    "        print(f\"Offending contrastive label: {contr_labels}\")\n",
    "        break\n",
    "\n",
    "    # Verify range\n",
    "    num_classes = len(clf_classes)\n",
    "    if class_labels.min() < 0 or class_labels.max() >= num_classes:\n",
    "        print(f\"Error: class_labels out of range [0, {num_classes - 1}].\")\n",
    "        print(f\"class_labels: {class_labels.tolist()}\")\n",
    "\n",
    "    # Sanity check shapes\n",
    "    print(f\"Image batch shape: {images.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}, class_labels shape: {class_labels.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contrastive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
