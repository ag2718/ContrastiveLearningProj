{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hkZFEj3EsxEN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/healthy-ml/scratch/abinitha/miniconda3/envs/contrastive/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Installs needed:\n",
    "pip install datasets\n",
    "pip install torchvision\n",
    "'''\n",
    "import os\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "#from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from tree import Tree\n",
    "\n",
    "dogs = [\n",
    "    1,  # \"Hunting Dog\"\n",
    "    [\n",
    "        2,  # \"Sporting Dog\"\n",
    "        [\n",
    "            3,  # \"Spaniel\"\n",
    "            [156],  # Blenheim spaniel\n",
    "            [215],  # Brittany spaniel\n",
    "            [216],  # Clumber, clumber spaniel\n",
    "            [219],  # Cocker spaniel, English cocker spaniel, cocker\n",
    "            [217],  # English springer, English springer spaniel\n",
    "            [218],  # Welsh springer spaniel\n",
    "            [220],  # Sussex spaniel\n",
    "            [221],  # Irish water spaniel\n",
    "        ],\n",
    "        [\n",
    "            4,  # \"Retriever\"\n",
    "            [205],  # Flat-coated retriever\n",
    "            [206],  # Curly-coated retriever\n",
    "            [207],  # Golden retriever\n",
    "            [208],  # Labrador retriever\n",
    "            [209],  # Chesapeake Bay retriever\n",
    "        ],\n",
    "        [\n",
    "            5,  # \"Pointer\"\n",
    "            [210],  # German short-haired pointer\n",
    "            [211],  # Vizsla, Hungarian pointer\n",
    "        ],\n",
    "        [\n",
    "            6,  # \"Setter\"\n",
    "            [212],  # English setter\n",
    "            [213],  # Irish setter, red setter\n",
    "            [214],  # Gordon setter\n",
    "        ],\n",
    "    ],\n",
    "    [\n",
    "        7,  # \"Terrier\"\n",
    "        [\n",
    "            8,  # \"Wirehair\"\n",
    "            [189],  # Lakeland terrier\n",
    "            [190],  # Sealyham terrier, Sealyham\n",
    "        ],\n",
    "        [\n",
    "            9,  # \"Bullterrier\"\n",
    "            [179],  # Staffordshire bullterrier, Staffordshire bull terrier\n",
    "            [180],  # American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier\n",
    "        ],\n",
    "        [\n",
    "            10,  # \"Fox Terrier\"\n",
    "            [188],  # Wire-haired fox terrier\n",
    "        ],\n",
    "        [\n",
    "            11,  # \"Schnauzer\"\n",
    "            [196],  # Miniature schnauzer\n",
    "            [197],  # Giant schnauzer\n",
    "            [198],  # Standard schnauzer\n",
    "        ],\n",
    "        [191],  # Airedale, Airedale terrier\n",
    "        [193],  # Australian terrier\n",
    "        [181],  # Bedlington terrier\n",
    "        [182],  # Border terrier\n",
    "        [192],  # Cairn, cairn terrier\n",
    "        [194],  # Dandie Dinmont, Dandie Dinmont terrier\n",
    "        [195],  # Boston bull, Boston terrier\n",
    "        [184],  # Irish terrier\n",
    "        [183],  # Kerry blue terrier\n",
    "        [185],  # Norfolk terrier\n",
    "        [186],  # Norwich terrier\n",
    "        [199],  # Scotch terrier, Scottish terrier, Scottie\n",
    "        [200],  # Tibetan terrier, chrysanthemum dog\n",
    "        [201],  # Silky terrier, Sydney silky\n",
    "        [202],  # Soft-coated wheaten terrier\n",
    "        [203],  # West Highland white terrier\n",
    "        [187],  # Yorkshire terrier\n",
    "    ],\n",
    "    [\n",
    "        12,  # \"Hound\"\n",
    "        [\n",
    "            13,  # \"Coonhound\"\n",
    "            [165],  # Black-and-tan coonhound\n",
    "            [166],  # Walker hound, Walker foxhound\n",
    "        ],\n",
    "        [\n",
    "            14,  # \"Foxhound\"\n",
    "            [167],  # English foxhound\n",
    "            [168],  # Redbone\n",
    "        ],\n",
    "        [\n",
    "            15,  # \"Greyhound\"\n",
    "            [171],  # Italian greyhound\n",
    "            [172],  # Whippet\n",
    "        ],\n",
    "        [\n",
    "            16,  # \"Wolfhound\"\n",
    "            [169],  # Borzoi, Russian wolfhound\n",
    "            [170],  # Irish wolfhound\n",
    "        ],\n",
    "        [\n",
    "            17,  # \"Other Hounds\"\n",
    "            [160],  # Afghan hound, Afghan\n",
    "            [161],  # Basset, basset hound\n",
    "            [162],  # Beagle\n",
    "            [163],  # Bloodhound, sleuthhound\n",
    "            [164],  # Bluetick\n",
    "            [173],  # Ibizan hound, Ibizan Podenco\n",
    "            [174],  # Norwegian elkhound, elkhound\n",
    "            [175],  # Otterhound, otter hound\n",
    "            [176],  # Saluki, gazelle hound\n",
    "            [177],  # Scottish deerhound, deerhound\n",
    "            [178],  # Weimaraner\n",
    "            [159],  # Rhodesian ridgeback\n",
    "        ],\n",
    "    ],\n",
    "]\n",
    "\n",
    "def calculate_num_classes(array):\n",
    "    if isinstance(array[0], int):\n",
    "        return len(array)\n",
    "    return sum(calculate_num_classes(subarray) for subarray in array)\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    train_ds = load_dataset(\"imagenet-1k\", split='train', streaming=True, trust_remote_code=True)\n",
    "    val_ds = load_dataset(\"imagenet-1k\", split='validation', streaming=True, trust_remote_code=True)\n",
    "\n",
    "    # List of dog class indices in ImageNet\n",
    "    num_total_classes = calculate_num_classes(dogs)\n",
    "    class_tree =  Tree(dogs)\n",
    "    dog_classes = np.array(class_tree.nodes_at_depth(num_total_classes))\n",
    "    print (\"Dog Classes\")\n",
    "    print (dog_classes)\n",
    "    print (len(dog_classes))\n",
    "\n",
    "    # Define transformations for the dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),  # Ensure 3 channels\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "    dog_dataset = train_ds.filter(lambda example: example['label'] in dog_classes)\n",
    "\n",
    "    dog_dataset_val = val_ds.filter(lambda example: example['label'] in dog_classes)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Custom collate function to apply transformations on streaming data.\n",
    "        \"\"\"\n",
    "        images, labels = [], []\n",
    "        for example in batch:\n",
    "            image = transform(example['image'])  # Apply transformations\n",
    "            images.append(image)\n",
    "            labels.append(example['label'])\n",
    "        return torch.stack(images), torch.tensor(labels)\n",
    "\n",
    "    # Create a DataLoader for the filtered dataset\n",
    "    train_dataloader = DataLoader(dog_dataset, batch_size=30, num_workers=4, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(dog_dataset_val, batch_size=30, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "    return train_dataloader, val_dataloader, dogs, num_total_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_RtxDa-xb1f8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "from tree import Tree\n",
    "\n",
    "class_tree = None\n",
    "distance_matrix = None\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# use conda install pytorch torchvision -c pytorch in python env to import; if using Colab probably use pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BryRbbvOxxYN"
   },
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WgnKGa6TSWZd",
    "outputId": "2a165e16-a774-4c94-f617-d1b9f64ac731"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog Classes\n",
      "[156 215 216 219 217 218 220 221 205 206 207 208 209 210 211 212 213 214\n",
      " 189 190 179 180 188 196 197 198 191 193 181 182 192 194 195 184 183 185\n",
      " 186 199 200 201 202 203 187 165 166 167 168 171 172 169 170 160 161 162\n",
      " 163 164 173 174 175 176 177 178 159]\n",
      "63\n",
      "Dogs\n",
      "[1, [2, [3, [156], [215], [216], [219], [217], [218], [220], [221]], [4, [205], [206], [207], [208], [209]], [5, [210], [211]], [6, [212], [213], [214]]], [7, [8, [189], [190]], [9, [179], [180]], [10, [188]], [11, [196], [197], [198]], [191], [193], [181], [182], [192], [194], [195], [184], [183], [185], [186], [199], [200], [201], [202], [203], [187]], [12, [13, [165], [166]], [14, [167], [168]], [15, [171], [172]], [16, [169], [170]], [17, [160], [161], [162], [163], [164], [173], [174], [175], [176], [177], [178], [159]]]]\n",
      "Num total classes\n",
      "4\n",
      "Contrastive classes\n",
      "[3, 4, 5, 6, 8, 9, 10, 11, 191, 193, 181, 182, 192, 194, 195, 184, 183, 185, 186, 199, 200, 201, 202, 203, 187, 13, 14, 15, 16, 17]\n",
      "Clf classes\n",
      "[156, 215, 216, 219, 217, 218, 220, 221, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 189, 190, 179, 180, 188, 196, 197, 198, 191, 193, 181, 182, 192, 194, 195, 184, 183, 185, 186, 199, 200, 201, 202, 203, 187, 165, 166, 167, 168, 171, 172, 169, 170, 160, 161, 162, 163, 164, 173, 174, 175, 176, 177, 178, 159]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/healthy-ml/scratch/abinitha/miniconda3/envs/contrastive/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data/healthy-ml/scratch/abinitha/miniconda3/envs/contrastive/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n",
      "Distance matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1683780/710963703.py:150: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  distance_matrix = torch.tensor(precompute_lca_distances(np.arange(len(contrastive_classes)), contrastive_classes, contrastive_class_to_id, class_tree), device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 2., 2., 2., 2., 2., 2.],\n",
      "        [1., 0., 1., 1., 2., 2., 2., 2., 2., 2.],\n",
      "        [1., 1., 0., 1., 2., 2., 2., 2., 2., 2.],\n",
      "        [1., 1., 1., 0., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 0., 1., 1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 1., 0., 1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 1., 1., 0., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 1., 1., 1., 0., 1., 1.],\n",
      "        [2., 2., 2., 2., 1., 1., 1., 1., 0., 1.],\n",
      "        [2., 2., 2., 2., 1., 1., 1., 1., 1., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def precompute_lca_distances(labels, contrastive_classes, contrastive_class_to_id, class_tree):\n",
    "    print (labels.shape)\n",
    "    num_labels = labels.shape[0]\n",
    "    lca_matrix = torch.zeros((num_labels, num_labels), dtype=torch.long)\n",
    "    distance_matrix = torch.zeros((num_labels, num_labels),  dtype=torch.float)\n",
    "\n",
    "    # Precompute LCAs and distances\n",
    "    for i in range(num_labels):\n",
    "        for j in range(i + 1, num_labels):\n",
    "\n",
    "            class_i = contrastive_classes[labels[i].item()]\n",
    "            class_j = contrastive_classes[labels[j].item()]\n",
    "            lca = class_tree.find_lca(class_i, class_j)\n",
    "            distance_i = class_tree.find_distance_to_ancestor(class_i, lca)\n",
    "            distance_j = class_tree.find_distance_to_ancestor(class_j, lca)\n",
    "            min_distance = min(distance_i, distance_j)\n",
    "            lca_matrix[i, j] = lca\n",
    "            lca_matrix[j, i] = lca\n",
    "            distance_matrix[i, j] = min_distance\n",
    "            distance_matrix[j, i] = min_distance\n",
    "\n",
    "    return distance_matrix\n",
    "\n",
    "# Basic Contrastive Learning Model\n",
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_dim=128):\n",
    "        super(ContrastiveModel, self).__init__()\n",
    "        # Using ResNet backbone since we are using ImageNet and therefore compatible, feel free to change\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(num_features, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)  # Extract features\n",
    "        embeddings = self.projection_head(features)  # Project embeddings\n",
    "        logits = self.classifier(embeddings)  # Classification logits\n",
    "        return embeddings, logits\n",
    "\n",
    "\n",
    "# Define the NCE Loss\n",
    "class NCELoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07, dist_func_param=1):\n",
    "        super(NCELoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dist_func_param = dist_func_param\n",
    "\n",
    "    def forward(self, embeddings, labels):\n",
    "        global distance_matrix\n",
    "        # Normalize embeddings to unit vectors\n",
    "        embeddings = nn.functional.normalize(embeddings, dim=1)\n",
    "\n",
    "        similarity_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature\n",
    "\n",
    "        # Mask out self-similarity\n",
    "        mask = torch.eye(similarity_matrix.size(0), device=similarity_matrix.device).bool()\n",
    "        similarity_matrix = similarity_matrix.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        # Create targets: positive samples have the same label\n",
    "        #labels = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "        #positives = labels.float()\n",
    "        distances = torch.exp(-self.dist_func_param * distance_matrix[labels][:, labels])\n",
    "\n",
    "        #print (\"filtered distances\")\n",
    "        #print (distances[:10,:10])\n",
    "        #print (\"similarity matrix\")\n",
    "        #print (similarity_matrix[:10,:10])\n",
    "\n",
    "        # Compute log-softmax and NCE loss\n",
    "        log_prob = nn.functional.log_softmax(similarity_matrix, dim=1)\n",
    "        #print (\"Log prob\")\n",
    "        #print (log_prob[:10,:10])\n",
    "        loss_matrix = log_prob * distances\n",
    "\n",
    "        loss_matrix.fill_diagonal_(0)\n",
    "\n",
    "        #loss = -torch.sum(log_prob * positives) / labels.sum() #loss is only how far apart the positives are\n",
    "        loss = -torch.sum(loss_matrix) / loss_matrix.shape[0] # / labels.sum() #loss is only how far apart the positives are\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Sample dataset preparation, making flexible\n",
    "# TODO: make as function of level of specificity\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, size=100, num_classes=10, transform=None):\n",
    "        self.size = size\n",
    "        self.num_classes = num_classes\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Random RGB image and label\n",
    "        image = torch.rand(3, 224, 224)\n",
    "        label = idx % self.num_classes\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "'''\n",
    "# DataLoader setup\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "dataset = SampleDataset(size=100, num_classes=10, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "num_classes = 10'''\n",
    "\n",
    "train_dataloader, val_dataloader, dogs, num_total_classes = load_data()\n",
    "print (\"Dogs\")\n",
    "print (dogs)\n",
    "print (\"Num total classes\")\n",
    "print (num_total_classes)\n",
    "class_tree = Tree(dogs)\n",
    "\n",
    "contrastive_classes_specificity = 2\n",
    "contrastive_classes = class_tree.nodes_at_depth(contrastive_classes_specificity)\n",
    "contrastive_class_to_id = {_cls: i for i, _cls in enumerate(contrastive_classes)}\n",
    "\n",
    "clf_classes_specificity = 4\n",
    "clf_classes = class_tree.nodes_at_depth(clf_classes_specificity)\n",
    "clf_class_to_id = {_cls: i for i, _cls in enumerate(clf_classes)}\n",
    "\n",
    "print (\"Contrastive classes\")\n",
    "print (contrastive_classes)\n",
    "print (\"Clf classes\")\n",
    "print (clf_classes)\n",
    "\n",
    "num_classes = len(clf_classes)\n",
    "\n",
    "model = ContrastiveModel(num_classes).cuda()\n",
    "nce_loss_fn = NCELoss().cuda() # Generally probably add all of this to Colab to use the GPU\n",
    "classification_loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Precompute distance matrix\n",
    "distance_matrix = torch.tensor(precompute_lca_distances(np.arange(len(contrastive_classes)), contrastive_classes, contrastive_class_to_id, class_tree), device=device)\n",
    "\n",
    "print (\"Distance matrix\")\n",
    "print (distance_matrix[:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog Classes\n",
      "[156 215 216 219 217 218 220 221 205 206 207 208 209 210 211 212 213 214\n",
      " 189 190 179 180 188 196 197 198 191 193 181 182 192 194 195 184 183 185\n",
      " 186 199 200 201 202 203 187 165 166 167 168 171 172 169 170 160 161 162\n",
      " 163 164 173 174 175 176 177 178 159]\n",
      "63\n",
      "(3,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1683780/1807833294.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  distance_matrix = torch.tensor(precompute_lca_distances(np.arange(len(contrastive_classes)), contrastive_classes, contrastive_class_to_id, class_tree), device=device)\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, dogs, num_total_classes = load_data()\n",
    "class_tree = Tree(dogs)\n",
    "\n",
    "contrastive_classes_specificity = 1\n",
    "contrastive_classes = class_tree.nodes_at_depth(contrastive_classes_specificity)\n",
    "contrastive_class_to_id = {_cls: i for i, _cls in enumerate(contrastive_classes)}\n",
    "\n",
    "clf_classes_specificity = 3\n",
    "clf_classes = class_tree.nodes_at_depth(clf_classes_specificity)\n",
    "clf_class_to_id = {_cls: i for i, _cls in enumerate(clf_classes)}\n",
    "\n",
    "num_classes = len(clf_classes)\n",
    "\n",
    "model = ContrastiveModel(num_classes).cuda()\n",
    "nce_loss_fn = NCELoss().cuda() # Generally probably add all of this to Colab to use the GPU\n",
    "classification_loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Precompute distance matrix\n",
    "distance_matrix = torch.tensor(precompute_lca_distances(np.arange(len(contrastive_classes)), contrastive_classes, contrastive_class_to_id, class_tree), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch\n",
    "\n",
    "def calculate_metrics(predictions, labels, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculates precision, recall, and F1-score.\n",
    "    \"\"\"\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    precision = precision_score(labels, predictions, average=average, zero_division=0)\n",
    "    recall = recall_score(labels, predictions, average=average, zero_division=0)\n",
    "    f1 = f1_score(labels, predictions, average=average, zero_division=0)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def evaluate_model(model, dataloader, loss_fn, device, num_classes):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a given dataloader and returns classification statistics.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradients for evaluation\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            labels = torch.tensor([class_tree.which_ancestor(label.item(), clf_classes) for label in labels], device='cuda')\n",
    "            labels = torch.tensor([clf_class_to_id[clf_label.item()] for clf_label in labels], device='cuda')\n",
    "\n",
    "            # Forward pass\n",
    "            _, logits = model(images)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            # Update loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Predictions and accuracy\n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            total_correct += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            # Store predictions and labels for metric computation\n",
    "            all_predictions.append(predictions)\n",
    "            all_labels.append(labels)\n",
    "            num_batches += 1\n",
    "\n",
    "    # Combine all predictions and labels across batches\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # Compute metrics\n",
    "    precision, recall, f1 = calculate_metrics(all_predictions, all_labels)\n",
    "    conf_matrix = confusion_matrix(all_labels.cpu().numpy(), all_predictions.cpu().numpy())\n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    avg_loss = running_loss / num_batches\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, conf_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog Classes\n",
      "[156 215 216 219 217 218 220 221 205 206 207 208 209 210 211 212 213 214\n",
      " 189 190 179 180 188 196 197 198 191 193 181 182 192 194 195 184 183 185\n",
      " 186 199 200 201 202 203 187 165 166 167 168 171 172 169 170 160 161 162\n",
      " 163 164 173 174 175 176 177 178 159]\n",
      "63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/healthy-ml/scratch/abinitha/miniconda3/envs/contrastive/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data/healthy-ml/scratch/abinitha/miniconda3/envs/contrastive/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_1683780/1550603967.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  distance_matrix = torch.tensor(precompute_lca_distances(np.arange(len(contrastive_classes)), contrastive_classes, contrastive_class_to_id, class_tree), device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:11, ?it/s]\n",
      "Too many dataloader workers: 4 (max is dataset.num_shards=1). Stopping 3 dataloader workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]\n",
      "Train Loss: 9.2640, Train Accuracy: 0.0667\n",
      "Validation Loss: 3.4427, Validation Accuracy: 0.0000\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "Confusion Matrix:\n",
      "[[0 6 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 4 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 0.1667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:12, ?it/s]\n",
      "Too many dataloader workers: 4 (max is dataset.num_shards=1). Stopping 3 dataloader workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]\n",
      "Train Loss: 9.5206, Train Accuracy: 0.1667\n",
      "Validation Loss: 3.4267, Validation Accuracy: 0.0000\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "Confusion Matrix:\n",
      "[[0 6 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 4 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "\n",
    "train_dataloader, val_dataloader, dogs, num_total_classes = load_data()\n",
    "class_tree = Tree(dogs)\n",
    "\n",
    "contrastive_classes_specificity = 3\n",
    "contrastive_classes = class_tree.nodes_at_depth(contrastive_classes_specificity)\n",
    "contrastive_class_to_id = {_cls: i for i, _cls in enumerate(contrastive_classes)}\n",
    "\n",
    "clf_classes_specificity = 2\n",
    "clf_classes = class_tree.nodes_at_depth(clf_classes_specificity)\n",
    "clf_class_to_id = {_cls: i for i, _cls in enumerate(clf_classes)}\n",
    "\n",
    "num_classes = len(clf_classes)\n",
    "\n",
    "model = ContrastiveModel(num_classes).cuda()\n",
    "nce_loss_fn = NCELoss().cuda() # Generally probably add all of this to Colab to use the GPU\n",
    "classification_loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Precompute distance matrix\n",
    "distance_matrix = torch.tensor(precompute_lca_distances(np.arange(len(contrastive_classes)), contrastive_classes, contrastive_class_to_id, class_tree), device=device)\n",
    "\n",
    "for epoch in range(2):  # Using two epochs to test, adjust as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    num_batches = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        images, labels = batch\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        # Map labels to contrastive level\n",
    "        contr_labels = torch.tensor([class_tree.which_ancestor(label.item(), contrastive_classes) for label in labels], device='cuda')\n",
    "        contr_labels = torch.tensor([contrastive_class_to_id[contr_label.item()] for contr_label in contr_labels], device='cuda')\n",
    "\n",
    "        # Map labels to classification level\n",
    "        clf_labels = torch.tensor([class_tree.which_ancestor(label.item(), clf_classes) for label in labels], device='cuda')\n",
    "        clf_labels = torch.tensor([clf_class_to_id[clf_label.item()] for clf_label in clf_labels], device='cuda')\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings, logits = model(images)\n",
    "\n",
    "        # Compute losses\n",
    "        nce_loss = nce_loss_fn(embeddings, contr_labels) * len(contr_labels) / 75\n",
    "        classification_loss = classification_loss_fn(logits, clf_labels)\n",
    "        total_loss = nce_loss + classification_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running loss\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predictions = torch.max(logits, dim=1)  # Get class predictions\n",
    "        total_correct += (predictions == clf_labels).sum().item()  # Count correct predictions\n",
    "        total_samples += clf_labels.size(0)  # Update total samples\n",
    "        print(f\"Batch accuracy: {total_correct / total_samples:.4f}\")\n",
    "        num_batches += 1\n",
    "\n",
    "        all_predictions.append(predictions)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    epoch_loss = running_loss / num_batches\n",
    "    train_accuracy = total_correct / total_samples\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_accuracy, val_precision, val_recall, val_f1, val_conf_matrix = evaluate_model(\n",
    "        model, val_dataloader, classification_loss_fn, device, num_classes\n",
    "    )\n",
    "\n",
    "    # Log epoch statistics\n",
    "    print(f\"Epoch [{epoch + 1}/{2}]\")\n",
    "    print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1-Score: {val_f1:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch in range(args.num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    total_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings, logits = model(images)\n",
    "\n",
    "        # Compute classification loss\n",
    "        clf_labels = torch.tensor([class_tree.which_ancestor(label.item(), clf_classes) for label in labels], device=device)\n",
    "        clf_labels = torch.tensor([clf_class_to_id[clf_label.item()] for clf_label in clf_labels], device=device)\n",
    "        loss = classification_loss_fn(logits, clf_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update loss and accuracy\n",
    "        train_loss += loss.item()\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        total_correct_train += (predictions == clf_labels).sum().item()\n",
    "        total_samples_train += clf_labels.size(0)\n",
    "\n",
    "    # Compute training statistics\n",
    "    train_accuracy = total_correct_train / total_samples_train\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_accuracy, val_precision, val_recall, val_f1, val_conf_matrix = evaluate_model(\n",
    "        model, val_dataloader, classification_loss_fn, device, num_classes\n",
    "    )\n",
    "\n",
    "    # Log epoch statistics\n",
    "    print(f\"Epoch [{epoch + 1}/{args.num_epochs}]\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1-Score: {val_f1:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{val_conf_matrix}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-CQv1JHcz0i",
    "outputId": "7453257b-93c2-4595-f045-ee85f0faf755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCE, CLF Loss: 24.287818908691406 4.1379313468933105\n",
      "Total Loss: 28.425750732421875\n",
      "NCE, CLF Loss: 23.329919815063477 4.117332935333252\n",
      "Total Loss: 27.44725227355957\n",
      "NCE, CLF Loss: 24.186758041381836 4.11442756652832\n",
      "Total Loss: 28.301185607910156\n",
      "NCE, CLF Loss: 22.530248641967773 4.251932144165039\n",
      "Total Loss: 26.782180786132812\n",
      "NCE, CLF Loss: 22.803865432739258 4.272979736328125\n",
      "Total Loss: 27.076845169067383\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m      8\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#print (\"Batch\")\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#print (images.shape, labels.shape)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#print (labels)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/contrastive/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/contrastive/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/contrastive/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/contrastive/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/contrastive/lib/python3.12/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/miniconda3/envs/contrastive/lib/python3.12/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/contrastive/lib/python3.12/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m wait([\u001b[38;5;28mself\u001b[39m], timeout)\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/contrastive/lib/python3.12/multiprocessing/connection.py:1136\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m     ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniconda3/envs/contrastive/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# TODO: add in evals or call to eval.py file perhaps?\n",
    "for epoch in range(2):  # Using two epochs to test but add more epochs\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        images, labels = batch\n",
    "        #print (\"Batch\")\n",
    "        #print (images.shape, labels.shape)\n",
    "        #print (labels)\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        # for each image, find the label at the correct classification level\n",
    "        contr_labels = torch.tensor([class_tree.which_ancestor(label.item(), contrastive_classes) for label in labels], device='cuda')\n",
    "        contr_labels = torch.tensor([contrastive_class_to_id[contr_label.item()] for contr_label in contr_labels], device='cuda')\n",
    "        #print(\"CONTRASTIVE LABELS: \")\n",
    "        #print(contr_labels)\n",
    "\n",
    "        # Map labels to their IDs\n",
    "        clf_labels = torch.tensor([class_tree.which_ancestor(label.item(), clf_classes) for label in labels], device='cuda')\n",
    "        clf_labels = torch.tensor([clf_class_to_id[clf_label.item()] for clf_label in clf_labels], device='cuda')\n",
    "        #print (\"CLASS LABELS\")\n",
    "        #print (clf_labels)\n",
    "\n",
    "        # Print tensor shapes\n",
    "        '''print(f\"Images shape: {images.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        print(f\"Contrastive labels shape: {contr_labels.shape}\")\n",
    "        print(f\"Class labels shape: {clf_labels.shape}\")'''\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings, logits = model(images)\n",
    "\n",
    "        # Compute losses\n",
    "        nce_loss = nce_loss_fn(embeddings, contr_labels) * len(contr_labels) / 75\n",
    "        classification_loss = classification_loss_fn(logits, clf_labels)\n",
    "        total_loss = nce_loss + classification_loss  # Combined loss\n",
    "\n",
    "        # Print losses\n",
    "        print(f\"NCE, CLF Loss: {nce_loss} {classification_loss}\")\n",
    "        print(f\"Total Loss: {total_loss.item()}\")\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/2], Loss: {running_loss/len(train_dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Q2Bl3IpkyAD"
   },
   "outputs": [],
   "source": [
    "train_dat"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "contrastive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
